# **Hate Speech Detection using ML classifiers**
# **ML classifiers vs Neural Network**

#Insatlling necesaary libraries

!pip install scikit-optimize
!pip install scikit-learn
!pip install seaborn
!pip install xgboost
!pip install lightgbm
!pip install catboost
!pip install scikit-optimize
pip install pandas numpy seaborn matplotlib pycaret

#Cloning the source git
!git clone https://github.com/danushkhanna/iNeuron.ai-Phishing-Domain-Detection.git

#Importing the required libraries
import time
import pandas as pd
import numpy as np
import scipy as sp
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import sklearn.model_selection
from sklearn.preprocessing import LabelEncoder
from sklearn.manifold import TSNE

from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgbm
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from catboost import CatBoostClassifier
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
from skopt.plots import plot_convergence

import seaborn as sns
sns.set_style("darkgrid")

import warnings
warnings.filterwarnings("ignore")
import pickle
from pycaret.classification import *
# **Hate Speech Detection dataset**
from google.colab import drive
drive.mount('/content/drive')
df1=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CONAN_Dataset - CONAN_English_RewardTest.csv')
df2=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CONAN_Dataset - CONAN_English_RewardTrain.csv')
df2
df2.head()
print(df2.describe())
#SETTING UP THE DATA FOR MODELLING

setup(data=df2, target='class')
**Preparing Data for Modeling-**
X_train= df2['speech']
y_train= df2['class']
X_test= df1['speech']
y_test= df1['class']
from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming X_train is a pandas Series containing text data
# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit the vectorizer on the training data and transform it
X_train = tfidf_vectorizer.fit_transform(X_train)

# Transform the test data using the same vectorizer
X_test = tfidf_vectorizer.transform(X_test)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
# Define a dictionary to store the results
results = {}
## **Evaluating Various ML Models-**
**1.Logistic regression-**
%%time
start_time = time.time()
logistic = LogisticRegression()
logistic.fit(X_train, y_train)
y_pred = logistic.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['Logistic Regression'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['Logistic Regression'][0])
print("Precision:", results['Logistic Regression'][1])
print("Recall:", results['Logistic Regression'][2])
print("F1-score:", results['Logistic Regression'][3])
print("Training Time:", results['Logistic Regression'][4])
**2. K-Nearest Neighbors (KNN)**
%%time
start_time = time.time()
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['K-Nearest Neighbors (KNN)'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['K-Nearest Neighbors (KNN)'][0])
print("Precision:", results['K-Nearest Neighbors (KNN)'][1])
print("Recall:", results['K-Nearest Neighbors (KNN)'][2])
print("F1-score:", results['K-Nearest Neighbors (KNN)'][3])
print("Training Time:", results['K-Nearest Neighbors (KNN)'][4])
**3. Gaussian Naive Bayes (GaussianNB)**
gnb = GaussianNB()

# Train the classifier
start_time = time.time()
gnb.fit(X_train.toarray(), y_train)  # Convert sparse matrix to dense array
training_time = time.time() - start_time

# Predict on the test set
y_pred = gnb.predict(X_test.toarray())  # Convert sparse matrix to dense array

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Training Time:", training_time)

**4. Decision Trees**
%%time
start_time = time.time()
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['Decision Trees'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['Decision Trees'][0])
print("Precision:", results['Decision Trees'][1])
print("Recall:", results['Decision Trees'][2])
print("F1-score:", results['Decision Trees'][3])
print("Training Time:", results['Decision Trees'][4])
**5. Random Forest**
%%time
start_time = time.time()
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['Random Forest'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['Random Forest'][0])
print("Precision:", results['Random Forest'][1])
print("Recall:", results['Random Forest'][2])
print("F1-score:", results['Random Forest'][3])
print("Training Time:", results['Random Forest'][4])
**6. Extra Trees**
%%time
start_time = time.time()
et = ExtraTreesClassifier()
et.fit(X_train, y_train)
y_pred = et.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['Extra Trees'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['Extra Trees'][0])
print("Precision:", results['Extra Trees'][1])
print("Recall:", results['Extra Trees'][2])
print("F1-score:", results['Extra Trees'][3])
print("Training Time:", results['Extra Trees'][4])
**7. Support Vector Machines (SVM)**
%%time
start_time = time.time()
svm = SVC()
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['Support Vector Machines'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['Support Vector Machines'][0])
print("Precision:", results['Support Vector Machines'][1])
print("Recall:", results['Support Vector Machines'][2])
print("F1-score:", results['Support Vector Machines'][3])
print("Training Time:", results['Support Vector Machines'][4])
**8. Neural Network MLP (Multi-layer Perceptron)**
%%time
start_time = time.time()
mlp = MLPClassifier()
mlp.fit(X_train, y_train)
y_pred = mlp.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['Neural Networks (Multi-layer Perceptron)'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['Neural Networks (Multi-layer Perceptron)'][0])
print("Precision:", results['Neural Networks (Multi-layer Perceptron)'][1])
print("Recall:", results['Neural Networks (Multi-layer Perceptron)'][2])
print("F1-score:", results['Neural Networks (Multi-layer Perceptron)'][3])
print("Training Time:", results['Neural Networks (Multi-layer Perceptron)'][4])
**9. AdaBoost**
%%time
start_time = time.time()
ada = AdaBoostClassifier()
ada.fit(X_train, y_train)
y_pred = ada.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['AdaBoost'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['AdaBoost'][0])
print("Precision:", results['AdaBoost'][1])
print("Recall:", results['AdaBoost'][2])
print("F1-score:", results['AdaBoost'][3])
print("Training Time:", results['AdaBoost'][4])
**10. XGBoost**
%%time
start_time = time.time()
xgboost = xgb.XGBClassifier()
xgboost.fit(X_train, y_train)
y_pred = xgboost.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['XGBoost'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['XGBoost'][0])
print("Precision:", results['XGBoost'][1])
print("Recall:", results['XGBoost'][2])
print("F1-score:", results['XGBoost'][3])
print("Training Time:", results['XGBoost'][4])
**11. Light Gradient Boosting Machine (LGBM)**
import lightgbm as lgbm

# Initialize LightGBM classifier
lgbm_classifier = lgbm.LGBMClassifier()

# Train the classifier
start_time = time.time()
lgbm_classifier.fit(X_train, y_train)
training_time = time.time() - start_time

# Predict on the test set
y_pred = lgbm_classifier.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Training Time:", training_time)

**12. CatBoost**
%%time
start_time = time.time()
cat = CatBoostClassifier()
cat.fit(X_train, y_train)
y_pred = cat.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['CatBoost'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['CatBoost'][0])
print("Precision:", results['CatBoost'][1])
print("Recall:", results['CatBoost'][2])
print("F1-score:", results['CatBoost'][3])
print("Training Time:", results['CatBoost'][4])
**13. Stochastic Gradient Descent (SGD)**
%%time
start_time = time.time()
sgd = SGDClassifier()
sgd.fit(X_train, y_train)
y_pred = sgd.predict(X_test)

end_time = time.time()
training_time = end_time - start_time

results['Stochastic Gradient Descent (SGD)'] = [accuracy_score(y_test, y_pred),
                                  precision_score(y_test, y_pred, average='weighted'),
                                  recall_score(y_test, y_pred, average='weighted'),
                                  f1_score(y_test, y_pred, average='weighted'),
                                  training_time]
print("Accuracy:", results['Stochastic Gradient Descent (SGD)'][0])
print("Precision:", results['Stochastic Gradient Descent (SGD)'][1])
print("Recall:", results['Stochastic Gradient Descent (SGD)'][2])
print("F1-score:", results['Stochastic Gradient Descent (SGD)'][3])
print("Training Time:", results['Stochastic Gradient Descent (SGD)'][4])
**14. Linear Discriminant Analysis (LDA)**
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Initialize Linear Discriminant Analysis (LDA)
lda = LinearDiscriminantAnalysis()

# Train the classifier
start_time = time.time()
lda.fit(X_train.toarray(), y_train)  # Convert sparse matrix to dense array
training_time = time.time() - start_time

# Predict on the test set
y_pred = lda.predict(X_test.toarray())  # Convert sparse matrix to dense array

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Training Time:", training_time)

**15. Quadratic Discriminant Analysis (QDA)**
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

# Initialize Quadratic Discriminant Analysis (QDA)
qda = QuadraticDiscriminantAnalysis()

# Train the classifier
start_time = time.time()
qda.fit(X_train.toarray(), y_train)  # Convert sparse matrix to dense array
training_time = time.time() - start_time

# Predict on the test set
y_pred = qda.predict(X_test.toarray())  # Convert sparse matrix to dense array

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Training Time:", training_time)

## Neural Networks 🧠
creating NN layers and compiling with loss, metrics, optimizer configs
Training NN
import time
import tensorflow as tf
import numpy as np

# Convert SparseTensor to dense numpy arrays
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()

model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

model.compile(loss='binary_crossentropy', metrics=['binary_accuracy'], optimizer='adam')

start_time = time.time()

history = model.fit(X_train_dense,
                     y_train,
                     epochs=500,
                     verbose=1,
                     batch_size=32,
                     validation_data=(X_test_dense, y_test))

end_time = time.time()
training_time = end_time - start_time

plt.plot(history.history['val_binary_accuracy'])
plt.plot(history.history['binary_accuracy'])
y_pred = np.round(model.predict(X_test)).flatten()

results['Neural Network'] = [accuracy_score(y_test, y_pred),
                             precision_score(y_test, y_pred, average='weighted'),
                             recall_score(y_test, y_pred, average='weighted'),
                             f1_score(y_test, y_pred, average='weighted'),
                             training_time]
print("Accuracy:", results['Neural Network'][0])
print("Precision:", results['Neural Network'][1])
print("Recall:", results['Neural Network'][2])
print("F1-score:", results['Neural Network'][3])
print("Training Time:", results['Neural Network'][4])
## Model Result Comparison and Analysis 📈
df_results = pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Training Time'])

df_results
## Analysing Best Performing Model - Linear Discriminant Analysis (LDA)
%%time
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Convert SparseTensor to dense numpy arrays
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()

# Initialize Linear Discriminant Analysis (LDA)
lda = LinearDiscriminantAnalysis()

# Train the classifier
start_time = time.time()
lda.fit(X_train_dense, y_train)
training_time = time.time() - start_time

# Predict on the test set
y_pred = lda.predict(X_test_dense)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Training Time:", training_time)

from sklearn.metrics import classification_report

# Generate classification report
report = classification_report(y_test, y_pred)
print(report)
## Confusion Matrix-
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Define class labels
classes = ['0', '1']

# Plot confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Fill the cells of the confusion matrix with values
thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.show()
import numpy as np

# Iterate over each classifier
classifiers = {
    'Logistic Regression': logistic,
    'K-Nearest Neighbors (KNN)': knn,
    'Gaussian Naive Bayes': gnb,
    'Decision Trees': dt,
    'Random Forest': rf,
    'Extra Trees': et,
    'Support Vector Machines': svm,
    'Neural Networks (Multi-layer Perceptron)': mlp,
    'AdaBoost': ada,
    'XGBoost': xgboost,
    'LightGBM': lgbm,
    'CatBoost': cat,
    'Stochastic Gradient Descent (SGD)': sgd,
    'Linear Discriminant Analysis': lda,
    'Quadratic Discriminant Analysis': qda,
    'Neural Network': model
}

for name, classifier in classifiers.items():
    try:
        # Predict using the classifier
        if name == 'Neural Network':  # For neural network model
            # Convert SparseTensor to dense numpy arrays
            X_test_dense = X_test.toarray()
            y_pred = np.round(classifier.predict(X_test_dense)).flatten()
        elif name == 'LightGBM':
            # Predict using LightGBM model directly
            y_pred = classifier.predict(X_test)
        else:
            # Convert TF-IDF transformed sparse matrices to dense numpy arrays
            X_test_dense = X_test.toarray()
            y_pred = classifier.predict(X_test_dense)

        # Get indices of rows predicted as hate speech
        hate_speech_indices = np.where(y_pred == 1)[0]

        # Get the rows of speeches predicted as hate speech
        hate_speech_rows = df1.iloc[hate_speech_indices]

        # Display the hate speech rows
        print(f"Hate speech detected by {name} classifier:")
        print(hate_speech_rows)

        # Save the hate speech rows to a CSV file
        hate_speech_rows.to_csv(f"/content/drive/MyDrive/Colab Notebooks/{name}_hate_speech.csv", index=False)
    except AttributeError:
        print(f"AttributeError: Cannot predict using {name}.")
